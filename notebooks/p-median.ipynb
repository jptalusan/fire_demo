{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "607a6821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append( \"../\")\n",
    "from src.utils import solve_pmedian_with_balance, add_p_via_mip, relocate_via_mip, relocate_via_greedy,greedy_add,preprocess, add_p_via_mip_multi\n",
    "from src.models import SurvivalRegressionForecaster\n",
    "import importlib\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from itertools import product\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f23afaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OSRM base URL\n",
    "OSRM_URL = \"http://localhost:8080/route/v1/driving\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b817458",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_d=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55107b6",
   "metadata": {},
   "source": [
    "Todo\n",
    "\n",
    "Data preprocessing\n",
    "Multiple Firestations within a single grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307cf78e",
   "metadata": {},
   "source": [
    "LOADING DATASET and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a4503bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/7k8vj9w17715wv9yqsqn7pm00000gq/T/ipykernel_9422/1104123106.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  grids['x']= grids['geometry'].centroid.x\n",
      "/var/folders/tf/7k8vj9w17715wv9yqsqn7pm00000gq/T/ipykernel_9422/1104123106.py:3: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  grids['y']= grids['geometry'].centroid.y\n"
     ]
    }
   ],
   "source": [
    "grids= gpd.read_file('../sample_data/grids.shp')\n",
    "grids['x']= grids['geometry'].centroid.x\n",
    "grids['y']= grids['geometry'].centroid.y\n",
    "survival_data= pd.read_csv('../sample_data/survival_dataset.csv')\n",
    "fire_stations= gpd.read_file('../sample_data/Fire_Station_Locations_-1484530934421591558.geojson')\n",
    "fire_stations= gpd.sjoin(fire_stations, grids, how='inner', predicate='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a844e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=joblib.load('../models/model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d715c3",
   "metadata": {},
   "source": [
    "DATA Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29b12490",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat=['window']\n",
    "num=['historical_density']\n",
    "\n",
    "train_df,test_df,features,scaler =preprocess(survival_data, cat, num)\n",
    "train_df[features]=train_df[features].astype(float)\n",
    "test_df[features]=test_df[features].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9500d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b081069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# half_count = len(test_df) // 4\n",
    "# test_df.iloc[:half_count].to_csv('../sample_data/sample_incidents.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58b34b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>cell_id</th>\n",
       "      <th>weekday</th>\n",
       "      <th>historical_density</th>\n",
       "      <th>xd</th>\n",
       "      <th>weighted_population</th>\n",
       "      <th>weighted_housing_units</th>\n",
       "      <th>cluster_label</th>\n",
       "      <th>...</th>\n",
       "      <th>AlarmDate</th>\n",
       "      <th>PSAPDate</th>\n",
       "      <th>time_bet</th>\n",
       "      <th>window_1</th>\n",
       "      <th>window_2</th>\n",
       "      <th>window_3</th>\n",
       "      <th>window_4</th>\n",
       "      <th>window_5</th>\n",
       "      <th>original_historical_density</th>\n",
       "      <th>predicted_time_bet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-01 00:00:00</td>\n",
       "      <td>36.146819</td>\n",
       "      <td>-86.757259</td>\n",
       "      <td>323</td>\n",
       "      <td>0</td>\n",
       "      <td>0.483995</td>\n",
       "      <td>335</td>\n",
       "      <td>3668</td>\n",
       "      <td>1722</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-01 00:04:23</td>\n",
       "      <td>2023-10-31 23:27:00.000</td>\n",
       "      <td>8.363611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028573</td>\n",
       "      <td>2.298654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-01 00:00:00</td>\n",
       "      <td>36.172264</td>\n",
       "      <td>-86.815118</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.074961</td>\n",
       "      <td>40</td>\n",
       "      <td>3015</td>\n",
       "      <td>1283</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-01 00:07:02</td>\n",
       "      <td>2023-11-01 00:06:46.000</td>\n",
       "      <td>19.011111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015812</td>\n",
       "      <td>1.148734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-01 00:00:00</td>\n",
       "      <td>36.135756</td>\n",
       "      <td>-86.787771</td>\n",
       "      <td>270</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017206</td>\n",
       "      <td>122</td>\n",
       "      <td>6379</td>\n",
       "      <td>2523</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-01 00:22:33</td>\n",
       "      <td>2023-11-01 00:22:12.000</td>\n",
       "      <td>1.593056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017916</td>\n",
       "      <td>0.968673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-11-01 00:00:00</td>\n",
       "      <td>36.158489</td>\n",
       "      <td>-86.777128</td>\n",
       "      <td>297</td>\n",
       "      <td>0</td>\n",
       "      <td>2.765679</td>\n",
       "      <td>416</td>\n",
       "      <td>2310</td>\n",
       "      <td>2244</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-01 00:25:32</td>\n",
       "      <td>2023-11-01 00:25:13.000</td>\n",
       "      <td>0.825278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080664</td>\n",
       "      <td>0.811596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-01 00:00:00</td>\n",
       "      <td>36.192344</td>\n",
       "      <td>-86.729003</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.524330</td>\n",
       "      <td>44</td>\n",
       "      <td>2862</td>\n",
       "      <td>1639</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-01 00:40:31</td>\n",
       "      <td>2023-11-01 00:38:00.000</td>\n",
       "      <td>13.766667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005552</td>\n",
       "      <td>14.946698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18265</th>\n",
       "      <td>2023-12-31 23:00:00</td>\n",
       "      <td>36.160723</td>\n",
       "      <td>-86.778429</td>\n",
       "      <td>297</td>\n",
       "      <td>1</td>\n",
       "      <td>2.765679</td>\n",
       "      <td>416</td>\n",
       "      <td>2310</td>\n",
       "      <td>2244</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-12-31 23:20:18</td>\n",
       "      <td>2023-12-31 23:20:18.000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.080664</td>\n",
       "      <td>0.558395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18266</th>\n",
       "      <td>2023-12-31 23:00:00</td>\n",
       "      <td>36.160950</td>\n",
       "      <td>-86.775805</td>\n",
       "      <td>297</td>\n",
       "      <td>1</td>\n",
       "      <td>2.765679</td>\n",
       "      <td>416</td>\n",
       "      <td>2310</td>\n",
       "      <td>2244</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-12-31 23:21:48</td>\n",
       "      <td>2023-12-31 23:21:48.000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.080664</td>\n",
       "      <td>0.558395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18267</th>\n",
       "      <td>2023-12-31 23:00:00</td>\n",
       "      <td>36.161030</td>\n",
       "      <td>-86.777204</td>\n",
       "      <td>297</td>\n",
       "      <td>1</td>\n",
       "      <td>2.765679</td>\n",
       "      <td>416</td>\n",
       "      <td>2310</td>\n",
       "      <td>2244</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-12-31 23:28:28</td>\n",
       "      <td>2023-12-31 23:28:28.000</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.080664</td>\n",
       "      <td>0.558395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18268</th>\n",
       "      <td>2023-12-31 23:00:00</td>\n",
       "      <td>36.161589</td>\n",
       "      <td>-86.776560</td>\n",
       "      <td>297</td>\n",
       "      <td>1</td>\n",
       "      <td>2.765679</td>\n",
       "      <td>416</td>\n",
       "      <td>2310</td>\n",
       "      <td>2244</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-12-31 23:39:34</td>\n",
       "      <td>2023-12-31 23:39:34.000</td>\n",
       "      <td>0.143611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.080664</td>\n",
       "      <td>0.558395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18269</th>\n",
       "      <td>2023-12-31 23:00:00</td>\n",
       "      <td>36.160950</td>\n",
       "      <td>-86.775805</td>\n",
       "      <td>297</td>\n",
       "      <td>1</td>\n",
       "      <td>2.765679</td>\n",
       "      <td>416</td>\n",
       "      <td>2310</td>\n",
       "      <td>2244</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-12-31 23:48:11</td>\n",
       "      <td>2023-12-31 23:48:11.000</td>\n",
       "      <td>0.183889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.080664</td>\n",
       "      <td>0.558395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18270 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Time   Latitude  Longitude  cell_id  weekday  \\\n",
       "0      2023-11-01 00:00:00  36.146819 -86.757259      323        0   \n",
       "1      2023-11-01 00:00:00  36.172264 -86.815118      250        0   \n",
       "2      2023-11-01 00:00:00  36.135756 -86.787771      270        0   \n",
       "3      2023-11-01 00:00:00  36.158489 -86.777128      297        0   \n",
       "4      2023-11-01 00:00:00  36.192344 -86.729003      384        0   \n",
       "...                    ...        ...        ...      ...      ...   \n",
       "18265  2023-12-31 23:00:00  36.160723 -86.778429      297        1   \n",
       "18266  2023-12-31 23:00:00  36.160950 -86.775805      297        1   \n",
       "18267  2023-12-31 23:00:00  36.161030 -86.777204      297        1   \n",
       "18268  2023-12-31 23:00:00  36.161589 -86.776560      297        1   \n",
       "18269  2023-12-31 23:00:00  36.160950 -86.775805      297        1   \n",
       "\n",
       "       historical_density   xd  weighted_population  weighted_housing_units  \\\n",
       "0                0.483995  335                 3668                    1722   \n",
       "1               -0.074961   40                 3015                    1283   \n",
       "2                0.017206  122                 6379                    2523   \n",
       "3                2.765679  416                 2310                    2244   \n",
       "4               -0.524330   44                 2862                    1639   \n",
       "...                   ...  ...                  ...                     ...   \n",
       "18265            2.765679  416                 2310                    2244   \n",
       "18266            2.765679  416                 2310                    2244   \n",
       "18267            2.765679  416                 2310                    2244   \n",
       "18268            2.765679  416                 2310                    2244   \n",
       "18269            2.765679  416                 2310                    2244   \n",
       "\n",
       "       cluster_label  ...           AlarmDate                 PSAPDate  \\\n",
       "0                  5  ... 2023-11-01 00:04:23  2023-10-31 23:27:00.000   \n",
       "1                  7  ... 2023-11-01 00:07:02  2023-11-01 00:06:46.000   \n",
       "2                  7  ... 2023-11-01 00:22:33  2023-11-01 00:22:12.000   \n",
       "3                  2  ... 2023-11-01 00:25:32  2023-11-01 00:25:13.000   \n",
       "4                  3  ... 2023-11-01 00:40:31  2023-11-01 00:38:00.000   \n",
       "...              ...  ...                 ...                      ...   \n",
       "18265              2  ... 2023-12-31 23:20:18  2023-12-31 23:20:18.000   \n",
       "18266              2  ... 2023-12-31 23:21:48  2023-12-31 23:21:48.000   \n",
       "18267              2  ... 2023-12-31 23:28:28  2023-12-31 23:28:28.000   \n",
       "18268              2  ... 2023-12-31 23:39:34  2023-12-31 23:39:34.000   \n",
       "18269              2  ... 2023-12-31 23:48:11  2023-12-31 23:48:11.000   \n",
       "\n",
       "        time_bet  window_1  window_2  window_3  window_4 window_5  \\\n",
       "0       8.363611       0.0       0.0       0.0       0.0      0.0   \n",
       "1      19.011111       0.0       0.0       0.0       0.0      0.0   \n",
       "2       1.593056       0.0       0.0       0.0       0.0      0.0   \n",
       "3       0.825278       0.0       0.0       0.0       0.0      0.0   \n",
       "4      13.766667       0.0       0.0       0.0       0.0      0.0   \n",
       "...          ...       ...       ...       ...       ...      ...   \n",
       "18265   0.025000       0.0       0.0       0.0       0.0      1.0   \n",
       "18266   0.111111       0.0       0.0       0.0       0.0      1.0   \n",
       "18267   0.185000       0.0       0.0       0.0       0.0      1.0   \n",
       "18268   0.143611       0.0       0.0       0.0       0.0      1.0   \n",
       "18269   0.183889       0.0       0.0       0.0       0.0      1.0   \n",
       "\n",
       "       original_historical_density  predicted_time_bet  \n",
       "0                         0.028573            2.298654  \n",
       "1                         0.015812            1.148734  \n",
       "2                         0.017916            0.968673  \n",
       "3                         0.080664            0.811596  \n",
       "4                         0.005552           14.946698  \n",
       "...                            ...                 ...  \n",
       "18265                     0.080664            0.558395  \n",
       "18266                     0.080664            0.558395  \n",
       "18267                     0.080664            0.558395  \n",
       "18268                     0.080664            0.558395  \n",
       "18269                     0.080664            0.558395  \n",
       "\n",
       "[18270 rows x 35 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.predict(train_df,{'features':features})\n",
    "model.predict(test_df,{'features':features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c566508d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6807dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "OD_matrix=False\n",
    "if OD_matrix:\n",
    "    pairs = [(i,j) for i in test_df.index for j in grids.index]\n",
    "\n",
    "    # Create merged DataFrame\n",
    "    data = []\n",
    "    for i_idx, f_idx in tqdm(pairs, total=len(pairs)):\n",
    "        data.append({\n",
    "            \"incident_id\": i_idx,\n",
    "            \"incident_lat\": test_df.at[i_idx, 'Latitude'],\n",
    "            \"incident_lon\": test_df.at[i_idx, 'Longitude'],\n",
    "            \"fire_station_id\": f_idx,\n",
    "            \"fire_lat\": grids.at[f_idx, 'y'],\n",
    "            \"fire_lon\": grids.at[f_idx, 'x']\n",
    "        })\n",
    "\n",
    "    df_merged = pd.DataFrame(data)\n",
    "\n",
    "    print(\"Merged DataFrame shape:\", df_merged.shape)\n",
    "    # Show a sample\n",
    "    df_merged.head()\n",
    "\n",
    "\n",
    "    # ————— Configuration —————\n",
    "\n",
    "    MAX_WORKERS     = 10\n",
    "    REQUEST_TIMEOUT = 5  # seconds\n",
    "\n",
    "    # ————— Helper functions —————\n",
    "    def build_osrm_url(o_lon, o_lat, d_lon, d_lat):\n",
    "        return f\"{OSRM_URL}/{o_lon},{o_lat};{d_lon},{d_lat}?overview=false\"\n",
    "\n",
    "    def get_travel_time_from_row(row: dict):\n",
    "        \"\"\"\n",
    "        row must be a dict with keys\n",
    "        'fire_lon','fire_lat','incident_lon','incident_lat'\n",
    "        \"\"\"\n",
    "        url = build_osrm_url(\n",
    "            row['fire_lon'], row['fire_lat'],\n",
    "            row['incident_lon'], row['incident_lat']\n",
    "        )\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=REQUEST_TIMEOUT)\n",
    "            resp.raise_for_status()\n",
    "            return resp.json()['routes'][0]['duration']\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # ————— Prepare inputs —————\n",
    "    rows = df_merged[\n",
    "        ['fire_lon','fire_lat','incident_lon','incident_lat']\n",
    "    ].to_dict('records')\n",
    "    travel_times = [None] * len(rows)\n",
    "\n",
    "    # ————— Parallel fetch via map + tqdm —————\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        for i, duration in enumerate(\n",
    "            tqdm(\n",
    "                executor.map(get_travel_time_from_row, rows),\n",
    "                total=len(rows),\n",
    "                desc=\"OSRM calls\"\n",
    "            )\n",
    "        ):\n",
    "            travel_times[i] = duration\n",
    "\n",
    "    # ————— Attach results —————\n",
    "    df_merged['travel_time_sec'] = travel_times\n",
    "    df_merged['travel_time_min'] = df_merged['travel_time_sec'] / 60\n",
    "\n",
    "    df_merged.to_csv('../sample_data/test_travel_times.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7701b281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/7k8vj9w17715wv9yqsqn7pm00000gq/T/ipykernel_9422/2585815586.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_merged['travel_time_sec'].fillna(1000000,inplace=True)\n",
      "/var/folders/tf/7k8vj9w17715wv9yqsqn7pm00000gq/T/ipykernel_9422/2585815586.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_merged['travel_time_min'].fillna(1000000/60,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_merged= pd.read_csv('../sample_data/test_travel_times.csv')\n",
    "df_merged['travel_time_sec'].fillna(1000000,inplace=True)\n",
    "df_merged['travel_time_min'].fillna(1000000/60,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a0bf79",
   "metadata": {},
   "source": [
    "P-median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac796fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "L=list(grids['cell_id'])\n",
    "X_exist= list(fire_stations['cell_id'])\n",
    "# --- toy example ---\n",
    "E = list(range(len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a={}\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    cluster_label = row['cluster_label']\n",
    "    if cluster_label in model.model_params:\n",
    "        w = np.array(model.model_params[cluster_label])\n",
    "        x = np.array(row[features])\n",
    "        w_x = np.dot(x, w.T)\n",
    "        log_t = np.log(row['time_bet'])\n",
    "        \n",
    "        # diff = log_t - w_x\n",
    "        diff=np.clip(log_t - w_x, -500, 500).item()\n",
    "        likelihood = (diff - np.exp(diff)).sum()\n",
    "        a[index] = np.exp(likelihood)\n",
    "    else:\n",
    "        a[index](np.nan)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, urllib.parse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from arcgis.gis import GIS\n",
    "\n",
    "gis = GIS(\"https://www.arcgis.com\", client_id='SpyQX09Wk1E27Afe',client_secret='da00d125df584efd8c96c8f7b36e1a49')\n",
    "\n",
    "# your two DataFrames\n",
    "# incident_df must have 'x' & 'y' columns; grid_df same\n",
    "incident_df = test_df.copy()\n",
    "grid_df     = grids.copy()\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://route.arcgis.com/arcgis/rest/services/World/Route/NAServer/Route_World/solve\"\n",
    "\n",
    "# travel mode JSON\n",
    "traffic_travel_mode = {\n",
    "    \"name\": \"Driving Time with Traffic\",\n",
    "    \"impedanceAttributeName\": \"TravelTime\",\n",
    "    \"attributeParameterValues\": [],\n",
    "    \"restrictions\": []\n",
    "}\n",
    "\n",
    "def fetch_travel_time(i, j):\n",
    "    # build the stops payload\n",
    "    start_iso = incident_df.at[i, 'AlarmDate'].isoformat()\n",
    "    stops = {\n",
    "      \"features\": [\n",
    "        {\"attributes\": {}, \"geometry\": {\n",
    "            \"x\": incident_df.at[i, 'Longitude'],\n",
    "            \"y\": incident_df.at[i, 'Latitude']\n",
    "        }},\n",
    "        {\"attributes\": {}, \"geometry\": {\n",
    "            \"x\": grid_df.at[j, 'x'],\n",
    "            \"y\": grid_df.at[j, 'y']\n",
    "        }}\n",
    "      ]\n",
    "    }\n",
    "    params = {\n",
    "      \"stops\"           : json.dumps(stops),\n",
    "      \"travelMode\"      : json.dumps(traffic_travel_mode),\n",
    "      \"startTime\"       : \"now\",\n",
    "      \"returnDirections\": \"false\",    # only need summary\n",
    "      \"f\"               : \"json\"\n",
    "    }\n",
    "    url = base_url + \"?\" + urllib.parse.urlencode(params)\n",
    "    resp = gis._con.get(url)\n",
    "\n",
    "    # pick up the travel time in minutes (field name may vary)\n",
    "    return resp['routes']['features'][0]['attributes']['Total_TravelTime']\n",
    "\n",
    "# build all (i,j) pairs\n",
    "pairs = [(i,j) for i in incident_df.index for j in grid_df.index]\n",
    "\n",
    "if old_d:\n",
    "    d={}\n",
    "    for index,row in df_merged.iterrows():\n",
    "        i=row['incident_id']\n",
    "        j=row['fire_station_id']\n",
    "        d[(i,j)]=row['travel_time_min']\n",
    "\n",
    "else:\n",
    "    d = {}\n",
    "    with ThreadPoolExecutor(max_workers=30) as pool:\n",
    "        futures = { pool.submit(fetch_travel_time, i, j): (i,j) for (i,j) in pairs }\n",
    "        for future in as_completed(futures):\n",
    "            i,j = futures[future]\n",
    "            try:\n",
    "                d[(i,j)] = future.result()\n",
    "            except Exception as e:\n",
    "                d[(i,j)] = None\n",
    "                print(f\"Error for {(i,j)}:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_loaded=joblib.load('../sample_data/args_pmedian/d.pkl')\n",
    "# X_exist_loaded=joblib.load('../sample_data/args_pmedian/X_exist.pkl')\n",
    "# L_loaded=joblib.load('../sample_data/args_pmedian/L.pkl')\n",
    "# a_loaded=joblib.load('../sample_data/args_pmedian/a.pkl')\n",
    "# E_loaded=joblib.load('../sample_data/args_pmedian/E.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by i,j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67460b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sort your dict items by (i, j)\n",
    "sorted_items = sorted(d.items(), key=lambda x: (x[0][0], x[0][1]))\n",
    "\n",
    "# 2. Extract the sorted list of unique i’s\n",
    "unique_i = sorted({i for (i, _), _ in sorted_items})\n",
    "\n",
    "# 3. Take the first half of those i’s\n",
    "half_count = len(unique_i) // 4\n",
    "first_i_set = set(unique_i[:half_count])\n",
    "\n",
    "# 4. Filter to only those entries whose i is in that first half\n",
    "first_half_by_i = {\n",
    "    (i, j): val\n",
    "    for (i, j), val in sorted_items\n",
    "    if i in first_i_set\n",
    "}\n",
    "\n",
    "a_sub = {k: a[k] for k in a if k < half_count}\n",
    "# model, X, Y, b = add_p_via_mip_multi(E[:half_count], L, a_sub, first_half_by_i, X_exist, p_add=0, alpha=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9df11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# joblib.dump(first_half_by_i, '../sample_data/args_pmedian/first_half_by_i.pkl')\n",
    "# joblib.dump(a_sub, '../sample_data/args_pmedian/a_sub.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728de101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subdict of d\n",
    "a_sub = {k: a[k] for k in a if k < half_count}\n",
    "# a_sub=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23db02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gurobipy as gp\n",
    "# from gurobipy import GRB\n",
    "# model2, X2, Y2, b = add_p_via_mip_multi(E, L, a, d, X_exist, p_add=1, alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f79b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, X, Y, b = add_p_via_mip_multi(E, L, a, d, X_exist, p_add=0, alpha=0)\n",
    "model, X, Y, b = add_p_via_mip_multi(E[:half_count], L, a_sub, first_half_by_i, X_exist, p_add=0, alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70317e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "def save_p_median_solution(\n",
    "    X, Y, \n",
    "    E, L,\n",
    "    output_dir=\".\",\n",
    "    prefix=\"solution\",\n",
    "    to_csv=True,\n",
    "    to_json=True,\n",
    "    to_pickle=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Persist a p-median MIP solution (X, Y) in various formats.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : dict-like of Gurobi vars\n",
    "        X[j].X gives the integer count at location j.\n",
    "    Y : dict-like of Gurobi vars\n",
    "        Y[i,j].X gives the binary assignment of demand i to location j.\n",
    "    E : iterable\n",
    "        Demand indices.\n",
    "    L : iterable\n",
    "        Candidate‐location indices.\n",
    "    output_dir : str, default=\".\"\n",
    "        Directory to write files into (will be created if necessary).\n",
    "    prefix : str, default=\"solution\"\n",
    "        Filename prefix (e.g. \"solution\" → \"solution_X.csv\", etc.).\n",
    "    to_csv : bool, default=True\n",
    "        Whether to write CSV files (`{prefix}_X.csv`, `{prefix}_Y.csv`).\n",
    "    to_json : bool, default=True\n",
    "        Whether to write `{prefix}.json`.\n",
    "    to_pickle : bool, default=True\n",
    "        Whether to write `{prefix}.pkl`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The Python objects that were saved: \n",
    "        {\n",
    "          \"X_vals\": {j: int},\n",
    "          \"Y_assign\": {i: j}\n",
    "        }\n",
    "    \"\"\"\n",
    "    # ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # extract solution values\n",
    "    X_vals = { j: int(X[j].X) for j in L }\n",
    "    Y_assign = {\n",
    "        i: j\n",
    "        for i in E\n",
    "        for j in L\n",
    "        if Y[i,j].X > 0.5\n",
    "    }\n",
    "\n",
    "    # CSV output\n",
    "    if to_csv:\n",
    "        df_X = pd.DataFrame.from_dict(\n",
    "            X_vals, orient=\"index\", columns=[\"count\"]\n",
    "        )\n",
    "        df_X.index.name = \"location\"\n",
    "        df_X.to_csv(os.path.join(output_dir, f\"{prefix}_X.csv\"))\n",
    "\n",
    "        df_Y = pd.DataFrame([\n",
    "            {\"demand\": i, \"location\": j}\n",
    "            for i, j in Y_assign.items()\n",
    "        ])\n",
    "        df_Y.to_csv(os.path.join(output_dir, f\"{prefix}_Y.csv\"), index=False)\n",
    "\n",
    "    # JSON output\n",
    "    if to_json:\n",
    "        with open(os.path.join(output_dir, f\"{prefix}.json\"), \"w\") as f:\n",
    "            json.dump({\"X\": X_vals, \"Y\": Y_assign}, f, indent=2)\n",
    "\n",
    "    # Pickle output\n",
    "    if to_pickle:\n",
    "        with open(os.path.join(output_dir, f\"{prefix}.pkl\"), \"wb\") as f:\n",
    "            pickle.dump((X_vals, Y_assign), f)\n",
    "\n",
    "    return {\"X_vals\": X_vals, \"Y_assign\": Y_assign}\n",
    "\n",
    "# model2.write(\"model2.lp\")\n",
    "model.write(\"model.lp\")\n",
    "# save_p_median_solution(X2, Y2, E, L, output_dir=\"../p_median\", prefix=\"solution2\")\n",
    "save_p_median_solution(X, Y, E, L, output_dir=\"../p_median\", prefix=\"solution\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic-anomaly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
